# CS231n

<<<<<<< HEAD
## Lecture 1

**IMAGENET**：Large Scale Visual Recognition Challenge

visual recognition problems thate are related to image classification:

* object detection
* image captioning

## Lecture 2

For images, weights of one class can form a new picture which may represent a templete for this class.

**Setting Hyperparameters**:

1. dataset can be spilt into three parts: training dataset、validation dataset、test dataset

2. Cross-Validation：split data into folds(only for training and setting hyperparameters)

   > useful for small datasets, but not used too frequently in deep learning.

A paper: Deep Visual-Semantic Alignments for Generating Image Descriptions CVPR 2015

## Lecture 3

**Two classifier loss functions:**

* Multiclass Support Vector Machine loss with hinge loss.
* softmax function

**Regularization**:

* L1 regularization
* L2 regularization
* Elastic net(L1 + L2)
* Max norm regularization
* Dropout
* Fancier: Batch normalization | stochastic depth

> debugging tools：Using numerical gradient to make sure that the analytic gradient is true。

## Lecture 4

**Backpropagation**: upstream gradient times local gradient.

> For example, if you multiplied all input data examples xi by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate.

Neural network is stacked up by many layers that are different non-linear functions.

## Lecture 5

**Fully Connected Layer**: streth all pixels out into a vector.

**Convolution Layer**: 32 * 32 * 3 image -> preserve spatial structure.

convolution layer output size: (N + P * 2   - F) / stride + 1

> N = input size, F = filter size, P = zeros padding
>
> In practice, Common to zero pad the border(with (F - 1) / 2 zeros).
>
> * valid convolution (0 padding)
> * same convolution
> * fully convolution (F - 1 padding)

**Pooling layer(no overlap)**:

* make the representations smaller and more manageable(downsample). 

* operates over each activation map independently.

  > one example: max pooling 

* its output size is the same as the Couvolution layer.

Converting FClayers to CONV layers

**Recent departures**: GoogleNet、ResNet

> in practice, use whatever works best on ImageNet.
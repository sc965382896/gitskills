# CS231n

## Lecture 1

**IMAGENET**：Large Scale Visual Recognition Challenge

visual recognition problems thate are related to image classification:

* object detection
* image captioning

## Lecture 2

For images, weights of one class can form a new picture which may represent a templete for this class.

**Setting Hyperparameters**:

1. dataset can be spilt into three parts: training dataset、validation dataset、test dataset

2. Cross-Validation：split data into folds(only for training and setting hyperparameters)

   > useful for small datasets, but not used too frequently in deep learning.

A paper: Deep Visual-Semantic Alignments for Generating Image Descriptions CVPR 2015

## Lecture 3

**Two classifier loss functions:**

* Multiclass Support Vector Machine loss with hinge loss.
* softmax function

**Regularization**:

* L1 regularization
* L2 regularization
* Elastic net(L1 + L2)
* Max norm regularization
* Dropout
* Fancier: Batch normalization | stochastic depth

> debugging tools：Using numerical gradient to make sure that the analytic gradient is true。

## Lecture 4

**Backpropagation**: upstream gradient times local gradient.

> For example, if you multiplied all input data examples xi by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate.

Neural network is stacked up by many layers that are different non-linear functions.

## Lecture 5

**Fully Connected Layer**: streth all pixels out into a vector.

**Convolution Layer**: 32 * 32 * 3 image -> preserve spatial structure.

convolution layer output size: (N + P * 2   - F) / stride + 1

> N = input size, F = filter size, P = zeros padding
>
> In practice, Common to zero pad the border(with (F - 1) / 2 zeros).
>
> * valid convolution (0 padding)
> * same convolution
> * fully convolution (F - 1 padding)

**Pooling layer(no overlap)**:

* make the representations smaller and more manageable(downsample). 

* operates over each activation map independently.

  > one example: max pooling 

* its output size is the same as the Couvolution layer.

Converting FClayers to CONV layers

**Recent departures**: GoogleNet、ResNet

> in practice, use whatever works best on ImageNet.

## Lecture 6

**Activation Functions**:

* Sigmoid: $\sigma(x)=1/(1+e^{-x})$
* tanh(x)
* ReLu(x): $f(x)=max(0,x)$
  * 计算效率高
  * 不会饱和
  * 收敛速度快
  * 存在dead ReLu，从而不会更新参数
* Leaky ReLu: $f(x)=max(0.01x,x)$

**Data preprocessing:**  

* Data Normalization
* Weight Initlization
* Batch Normalization

**Babysitting the Learning Process:**

1. Pre-process the data.
2. Choose the architecture.
3. Make sure that you can overfit very small portion of the training data.

   * take small amount of examples

   * turn off regularization

   * use sgd

4. Start with small regularization and find learning rate that makes the loss go down.

**Cross-validation strategy:**

* First stage: only afew epochs to get rough idea of what params work
* Second stage: longer running time, finer search

> **Tip:** if the cost is ever > 3 * original cost, break out early

**Hyperparameter Optimization:**

* random sample hyperparams
* in log space when appropriate

**Optimization Algrithm:**

* SGD
  * slow
  * local optimal
  * add noise
* SGD+Momentum
  * Overshotting
  * faster
* AdaGrad(for convex problem), ARMSprop(for Nerual Network)
  * better than two above

* Adam: Momentum + AdaGrad/RMSProp

**Regularization:**

* L1
* L2
* Dropout: in each forward pass, randomly set some neurons to zero
  * activation
  * in convolution network, entire channel